{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NPiymNVP35l"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UanUdPJfPybH"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import json\n",

        "\n",
        "\n",
        "def open_file(path, mode):\n",

        "  return open(path, mode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYmtr_VkQjDN"
      },
      "source": [
        "# Load Input File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNca9n1nQmyN"
      },
      "outputs": [],
      "source": [
        "# A path and filename with \"{}\" in place of the language variety name (from\n",
        "# LANGUAGES below).\n",

        "input_format_str = '{}.jsonl' # @param {type: 'string'}\n",
        "\n",
        "# Relevant keys within each JSONL entry.\n",
        "gold_answer_key = \"answer_texts\"  # @param [\"answer_texts\"] {allow-input: true}\n",
        "gold_answer_start_byte_indices_key = \"answer_start_byte_indices\" # @param [\"answer_start_byte_indices\"] {allow-input: true}\n",
        "gold_answer_end_byte_indices_key = \"answer_end_byte_indices\" # @param [\"answer_end_byte_indices\"] {allow-input: true}\n",
        "predicted_answer_key = \"generated_answer\" # @param [\"generated_answer\"] {allow-input: true}\n",
        "predicted_answer_byte_start_index_key = \"generated_answer_byte_start_index\" # @param [\"generated_answer_byte_start_index\"] {allow-input: true}\n",
        "predicted_answer_byte_end_index_key = \"generated_answer_byte_end_index\" # @param [\"generated_answer_byte_end_index\"] {allow-input: true}\n",

        "article_text_key = \"article_plaintext\" # @param [\"article_plaintext\"] {allow-input: true}\n",
        "answer_types_key = \"answer_types\"  # @param [\"answer_types\"] {allow-input: true}\n",
        "\n",
        "# Case-insensitive response strings for non-minimal-span answers.\n",
        "response_yes = \"yes\" # @param [\"yes\"] {allow-input: true}\n",
        "response_no = \"no\" # @param [\"no\"] {allow-input: true}\n",
        "response_null = \"no answer\" # @param [\"no answer\"] {allow-input: true}\n",
        "\n",

        "\n",
        "\n",
        "LANGUAGES = [\n",
        "    'tajik',\n",
        "    'farsi',\n",
        "    'arabic_iraq',\n",
        "    'arabic_jordan',\n",
        "    'azerbaijani',\n",
        "    'armenian',\n",
        "    'arabic_egypt',\n",
        "    'turkish',\n",
        "    'hebrew',\n",
        "    'arabic_algeria',\n",
        "]\n",
        "\n",
        "def apply_null_consensus(example):\n",
        "  \"\"\"Remove gold answers that don't align with NULL consensus.\n",
        "\n",
        "  If more than half of the gold answers are NULL, non-NULL answers are removed.\n",
        "  If more than half of the gold answers are non-NULL, NULL answers are removed.\n",
        "  \"\"\"\n",
        "  answer_types = example[answer_types_key]\n",
        "  null_consensus = answer_types.count('no_answer') \u003e len(answer_types) / 2\n",
        "  keep_mask = [(answer_type == 'no_answer') == null_consensus for answer_type in answer_types]\n",
        "\n",
        "  def _mask(elements):\n",
        "    return [element for element, keep in zip(elements, keep_mask) if keep]\n",
        "\n",
        "  for key in [gold_answer_key, gold_answer_start_byte_indices_key, gold_answer_end_byte_indices_key, answer_types_key]:\n",
        "    example[key] = _mask(example[key])\n",
        "\n",
        "  return example\n",
        "\n",
        "\n",
        "examples_by_language = defaultdict(list)\n",
        "for language in LANGUAGES:\n",


        "  input_filename = input_format_str.format(language)\n",
        "  with open_file(input_filename, 'r') as f:\n",
        "    for line in f:\n",
        "      example = apply_null_consensus(json.loads(line))\n",
        "      examples_by_language[language].append(example)\n",
        "  print(f'Loaded {len(examples_by_language[language])} inputs for {language}.')\n",











        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMqMSae-Vt-m"
      },
      "source": [
        "# Define Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMhZAvkjytwd"
      },
      "outputs": [],
      "source": [
        "def validate_predicted_answer_and_indices(\n",
        "    predicted_answer: str,\n",
        "    predicted_answer_byte_start_index: int | None,\n",
        "    predicted_answer_byte_end_index: int | None,\n",
        "    article_text_bytes: bytes,\n",
        ") -\u003e None:\n",
        "  provided_indices = [\n",
        "      predicted_answer_byte_start_index is not None,\n",
        "      predicted_answer_byte_end_index is not None,\n",
        "  ]\n",
        "  assert all(provided_indices) or not any(\n",
        "      provided_indices\n",
        "  ), \"Cannot provide only one predicted answer start/end index!\"\n",
        "  if all(provided_indices):\n",
        "    assert (\n",
        "        predicted_answer\n",
        "        == article_text_bytes[\n",
        "            predicted_answer_byte_start_index:predicted_answer_byte_end_index\n",
        "        ]\n",
        "    ), \"predicted_answer must be calculated from provided start/end indices!\"\n",
        "\n",
        "def f1_example(\n",
        "    gold_answer: str,\n",
        "    predicted_answer: str,\n",
        "    predicted_answer_byte_start_index: int | None,\n",
        "    predicted_answer_byte_end_index: int | None,\n",
        "    article_text: str,\n",
        "    answer_type: str,\n",
        "    gold_answer_start_byte_index: int,\n",
        "    gold_answer_end_byte_index: int,\n",
        ") -\u003e float:\n",
        "  \"\"\"Per-example F1 score.\"\"\"\n",
        "  article_text_bytes = article_text.encode(\"utf-8\")\n",
        "  predicted_answer_bytes = predicted_answer.strip().encode(\"utf-8\")\n",
        "\n",
        "  if answer_type == \"no_answer\":\n",
        "    return 1.0 if predicted_answer.lower().strip() == \"no answer\" else 0.0\n",
        "  elif answer_type == \"yes_no\":\n",
        "    return (\n",
        "        1.0 if predicted_answer.lower().strip() == gold_answer.lower().strip() else 0.0\n",
        "    )\n",
        "  elif answer_type == \"minimal_span\":\n",
        "    validate_predicted_answer_and_indices(\n",
        "        predicted_answer,\n",
        "        predicted_answer_byte_start_index,\n",
        "        predicted_answer_byte_end_index,\n",
        "        article_text_bytes,\n",
        "    )\n",
        "    predicted_start = (\n",
        "        article_text_bytes.find(predicted_answer_bytes)\n",
        "        if predicted_answer_byte_start_index is None\n",
        "        else predicted_answer_byte_start_index\n",
        "    )\n",
        "    if predicted_start == -1:\n",
        "      return 0.0\n",
        "    else:\n",
        "      predicted_end = (\n",
        "          predicted_start + len(predicted_answer_bytes)\n",
        "          if predicted_answer_byte_end_index is None\n",
        "          else predicted_answer_byte_end_index\n",
        "      )\n",
        "      predicted_indices = set(range(predicted_start, predicted_end))\n",
        "      gold_indices = set(\n",
        "          range(gold_answer_start_byte_index, gold_answer_end_byte_index)\n",
        "      )\n",
        "      tp2 = 2 * len(\n",
        "          predicted_indices.intersection(gold_indices)\n",
        "      )  # True positives * 2\n",
        "      fp = len(predicted_indices - gold_indices)  # False positives\n",
        "      fn = len(gold_indices - predicted_indices)  # False negatives\n",
        "      return tp2 / (tp2 + fp + fn)  # F1 = (2*TP) / ((2*TP) + FP + FN)\n",
        "  else:\n",
        "    raise NotImplementedError(f\"Unknown answer type: {answer_type}\")\n",
        "\n",
        "\n",
        "# F1 metric on eval set.\n",
        "def f1(\n",
        "    inputs: list[dict],\n",
        "    gold_answer_key: str,\n",
        "    gold_answer_start_byte_indices_key: str,\n",
        "    gold_answer_end_byte_indices_key: str,\n",
        "    predicted_answer_key: str,\n",
        "    predicted_answer_byte_start_index_key: str | None,\n",
        "    predicted_answer_byte_end_index_key: str | None,\n",
        "    article_text_key: str,\n",
        "    answer_types_key: str,\n",
        "    only_null: bool = False,\n",
        "    only_non_null: bool = False\n",
        ") -\u003e float:\n",
        "  assert not only_null or not only_non_null, \"Cannot specify both only_null and only_non_null!\"\n",
        "  scores = []\n",
        "  for item in inputs:\n",
        "    gold_answers = item[gold_answer_key]\n",
        "    article_text = item[article_text_key]\n",
        "    predicted_answer = item[predicted_answer_key]\n",
        "    predicted_answer_byte_start_index = (\n",
        "        item[predicted_answer_byte_start_index_key]\n",
        "        if predicted_answer_byte_start_index_key is not None\n",
        "        else None\n",
        "    )\n",
        "    predicted_answer_byte_end_index = (\n",
        "        item[predicted_answer_byte_end_index_key]\n",
        "        if predicted_answer_byte_end_index_key is not None\n",
        "        else None\n",
        "    )\n",
        "    answer_types = item[answer_types_key]\n",
        "    if 'no_answer' in set(answer_types):\n",
        "      if only_non_null:\n",
        "        continue\n",
        "    else:\n",
        "      if only_null:\n",
        "        continue\n",
        "    gold_start_byte_indices = item[gold_answer_start_byte_indices_key]\n",
        "    gold_end_byte_indices = item[gold_answer_end_byte_indices_key]\n",
        "    best_f1 = 0.0\n",
        "    for i, gold_answer in enumerate(gold_answers):\n",
        "      this_f1 = f1_example(\n",
        "          gold_answer,\n",
        "          predicted_answer,\n",
        "          predicted_answer_byte_start_index,\n",
        "          predicted_answer_byte_end_index,\n",
        "          article_text,\n",
        "          answer_types[i],\n",
        "          gold_start_byte_indices[i],\n",
        "          gold_end_byte_indices[i],\n",
        "      )\n",
        "      if this_f1 \u003e best_f1:\n",
        "        best_f1 = this_f1\n",
        "    scores.append(best_f1)\n",
        "\n",
        "  return sum(scores) / len(scores)\n",
        "\n",
        "def exact_match(\n",
        "    inputs: list[dict],\n",
        "    article_text_key: str,\n",
        "    gold_answer_key: str,\n",
        "    gold_answer_start_byte_indices_key: str,\n",
        "    gold_answer_end_byte_indices_key: str,\n",
        "    predicted_answer_key: str,\n",
        "    predicted_answer_byte_start_index_key: str | None,\n",
        "    predicted_answer_byte_end_index_key: str | None,\n",
        "    answer_types_key: str,\n",
        ") -\u003e float:\n",
        "  \"\"\"Exact Match (EM) metric on eval set. Same as F1, but no partial credit.\"\"\"\n",
        "  scores = []\n",
        "  for item in inputs:\n",
        "    gold_answers = item[gold_answer_key]\n",
        "    article_text = item[article_text_key]\n",
        "    predicted_answer = item[predicted_answer_key]\n",
        "    predicted_answer_byte_start_index = (\n",
        "        item[predicted_answer_byte_start_index_key]\n",
        "        if predicted_answer_byte_start_index_key is not None\n",
        "        else None\n",
        "    )\n",
        "    predicted_answer_byte_end_index = (\n",
        "        item[predicted_answer_byte_end_index_key]\n",
        "        if predicted_answer_byte_end_index_key is not None\n",
        "        else None\n",
        "    )\n",
        "    answer_types = item[answer_types_key]\n",
        "    gold_start_byte_indices = item[gold_answer_start_byte_indices_key]\n",
        "    gold_end_byte_indices = item[gold_answer_end_byte_indices_key]\n",
        "    best_f1 = 0.0\n",
        "    for i, gold_answer in enumerate(gold_answers):\n",
        "      this_f1 = f1_example(\n",
        "          gold_answer,\n",
        "          predicted_answer,\n",
        "          predicted_answer_byte_start_index,\n",
        "          predicted_answer_byte_end_index,\n",
        "          article_text,\n",
        "          answer_types[i],\n",
        "          gold_start_byte_indices[i],\n",
        "          gold_end_byte_indices[i],\n",
        "      )\n",
        "      if this_f1 \u003e best_f1:\n",
        "        best_f1 = this_f1\n",
        "    # Remove partial credit\n",
        "    if best_f1 \u003c 1.0:\n",
        "      best_f1 = 0.0\n",
        "    scores.append(best_f1)\n",
        "\n",
        "  return sum(scores) / len(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo-27Pmwytwe"
      },
      "source": [
        "# Evaluate the Inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ztf1tyFoytwe"
      },
      "outputs": [],
      "source": [
        "# If your model predictions already contain byte start/end indices, set this to\n",
        "# False. Otherwise, the first occurence of the predicted string in the article\n",
        "# will be used to infer the predicted indices.\n",
        "INFER_PREDICTED_INDICES = True # @param {type: \"boolean\"}\n",
        "\n",
        "metrics_by_language = defaultdict(dict)\n",
        "for language in LANGUAGES:\n",


        "  print(f\"{language}:\")\n",
        "  # Exact Match\n",
        "  exact_match_metric = exact_match(\n",
        "      examples_by_language[language],\n",
        "      article_text_key,\n",
        "      gold_answer_key,\n",
        "      gold_answer_start_byte_indices_key,\n",
        "      gold_answer_end_byte_indices_key,\n",
        "      predicted_answer_key,\n",
        "      None if INFER_PREDICTED_INDICES else predicted_answer_byte_start_index_key,\n",
        "      None if INFER_PREDICTED_INDICES else predicted_answer_byte_end_index_key,\n",
        "      answer_types_key,\n",
        "  )\n",
        "  metrics_by_language[language][\"exact_match\"] = exact_match_metric\n",
        "\n",
        "  # F1\n",
        "  f1_metric = f1(\n",
        "      examples_by_language[language],\n",
        "      gold_answer_key,\n",
        "      gold_answer_start_byte_indices_key,\n",
        "      gold_answer_end_byte_indices_key,\n",
        "      predicted_answer_key,\n",
        "      None if INFER_PREDICTED_INDICES else predicted_answer_byte_start_index_key,\n",
        "      None if INFER_PREDICTED_INDICES else predicted_answer_byte_end_index_key,\n",
        "      article_text_key,\n",
        "      answer_types_key,\n",
        "  )\n",
        "  metrics_by_language[language][\"f1\"] = f1_metric\n",
        "  f1_null_metric = f1(\n",
        "      examples_by_language[language],\n",
        "      gold_answer_key,\n",
        "      gold_answer_start_byte_indices_key,\n",
        "      gold_answer_end_byte_indices_key,\n",
        "      predicted_answer_key,\n",
        "      None if INFER_PREDICTED_INDICES else predicted_answer_byte_start_index_key,\n",
        "      None if INFER_PREDICTED_INDICES else predicted_answer_byte_end_index_key,\n",
        "      article_text_key,\n",
        "      answer_types_key,\n",
        "      only_null=True\n",
        "  )\n",
        "  f1_non_null_metric = f1(\n",
        "      examples_by_language[language],\n",
        "      gold_answer_key,\n",
        "      gold_answer_start_byte_indices_key,\n",
        "      gold_answer_end_byte_indices_key,\n",
        "      predicted_answer_key,\n",
        "      None if INFER_PREDICTED_INDICES else predicted_answer_byte_start_index_key,\n",
        "      None if INFER_PREDICTED_INDICES else predicted_answer_byte_end_index_key,\n",
        "      article_text_key,\n",
        "      answer_types_key,\n",
        "      only_non_null=True\n",
        "  )\n",
        "  metrics_by_language[language][\"f1_null\"] = f1_null_metric\n",
        "  metrics_by_language[language][\"f1_non_null\"] = f1_non_null_metric\n",
        "\n",
        "  print(metrics_by_language[language])\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNfJCnHMVAUT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
